import pandas as pd
import json
import re
from functools import partial
import subprocess as sp
from pathlib import Path
from os.path import basename, splitext, dirname
from snakemake.utils import min_version, validate
from scripts.common.config import lookup_run_json, lookup_config, lookup_run_config

min_version("6.12")


configfile: "config/config.yml"


validate(config, "schemas/config-schema.yml")


def get_git_tag():
    args = ["git", "describe", "--tags", "--abbrev=0", "--always"]
    tag = sp.run(args, capture_output=True).stdout.strip().decode()
    m = re.match("(v\d+\.\d+\.\d+)-\d+", tag)
    return tag if m is None else m[1]


git_tag = get_git_tag()
run_keys = list(config["ebm_runs"])

################################################################################
# output paths

conf_paths = config["paths"]

# downloaded files

resources_dir = Path(conf_paths["resources"])

bench_dir = resources_dir / "bench"
annotations_src_dir = resources_dir / "annotations"

# computed output

results_dir = Path(conf_paths["results"])

label_dir = results_dir / "labels"
rtg_dir = label_dir / "rtg"
annotations_tsv_dir = results_dir / "annotations"
annotated_dir = results_dir / "annotated_input"

ebm_dir = results_dir / "ebm" / "{}_{{run_key}}".format(git_tag)
ebm_output_files = [
    ebm_dir / f
    for f in [
        "model.pickle",
        "train_x.pickle",
        "train_y.pickle",
        "test_x.pickle",
        "test_y.pickle",
        "config.yml",
    ]
]

################################################################################
# main target
#
# Define what EBM models we want to run, and pin the output dirs to 'all'


rule all:
    input:
        expand(ebm_output_files, tag=git_tag, run_key=run_keys),


################################################################################
# VCF preprocessing


# rule get_vcf:
#     output:
#         join(resources_dir, "query.vcf.gz"),
#     params:
#         url=lookup_config(config, "resources", "query_url"),
#     shell:
#         "curl -o {output} {params.url}"


rule preprocess_vcf:
    input:
        resources_dir / "query.vcf.gz",
        # rules.get_vcf.output,
    output:
        label_dir / "query_corrected_refcall.vcf.gz",
    conda:
        "envs/samtools.yml"
    shell:
        """
        gunzip -c {input} | \
        sed -e '/.RefCall./ s/\.\/\./0\/1/g' | \
        sed -e '/.RefCall./ s/0\/0/0\/1/g' | \
        bgzip -c > {output}
        """


rule index_vcf:
    input:
        rules.preprocess_vcf.output,
    output:
        label_dir / "query_corrected_refcall.vcf.gz.tbi",
    conda:
        "envs/samtools.yml"
    shell:
        "tabix -p vcf {input}"


################################################################################
# get reference sdf


# TODO don't hardcode the reference in the future
ref_url = lookup_config(config, "resources", "references", "GRCh38", "sdf")


rule get_ref_sdf:
    output:
        directory(resources_dir / splitext(basename(ref_url))[0]),
    params:
        url=ref_url,
        dir=lambda _, output: dirname(output[0]),
    shell:
        "curl {params.url} | bsdtar -xf - -C {params.dir}"


################################################################################
# get benchmark files


def lookup_benchmark(key):
    # TODO don't hardcode the benchmark in the future
    return (lookup_config(config, "resources", "benchmarks", "v4.2.1", key),)


rule get_bench_vcf:
    output:
        bench_dir / "bench.vcf.gz",
    params:
        url=lookup_benchmark("vcf_url"),
    shell:
        "curl -o {output} {params.url}"


rule get_bench_tbi:
    output:
        bench_dir / "bench.vcf.gz.tbi",
    params:
        url=lookup_benchmark("tbi_url"),
    shell:
        "curl -o {output} {params.url}"


rule get_bench_bed:
    output:
        bench_dir / "bench.bed",
    params:
        url=lookup_benchmark("bed_url"),
    shell:
        "curl -o {output} {params.url}"


################################################################################
# VCF -> tsv


# rtg won't output to a directory that already exists, so do this weird temp
# file thing
rule get_vcf_labels:
    input:
        query_vcf=rules.preprocess_vcf.output,
        truth_vcf=rules.get_bench_vcf.output,
        truth_bed=rules.get_bench_bed.output,
        sdf=rules.get_ref_sdf.output,
        # not used on CLI but still needed
        query_tbi=rules.index_vcf.output,
        truth_tbi=rules.get_bench_tbi.output,
    output:
        tp=rtg_dir / "tp.vcf.gz",
        fp=rtg_dir / "fp.vcf.gz",
    conda:
        "envs/rtg.yml"
    params:
        extra="--ref-overlap --all-records",
        tmp_dir="/tmp/vcfeval",
        output_dir=rtg_dir,
    shell:
        """
        rtg vcfeval {params.extra} \
            -b {input.truth_vcf} \
            -e {input.truth_bed} \
            -c {input.query_vcf} \
            -o {params.tmp_dir} \
        -t {input.sdf}
        mv {params.tmp_dir}/* {params.output_dir}
        rm -r {params.tmp_dir}
        """


rule unzip_vcf_labels:
    input:
        rtg_dir / "{label}.vcf.gz",
    output:
        label_dir / "{label}.vcf",
    shell:
        "gunzip {input} -c > {output}"


rule parse_label_vcf:
    input:
        rules.unzip_vcf_labels.output,
    output:
        label_dir / "{filter_prefix}_{label}.tsv",
    shell:
        """
        python \
        workflow/scripts/parse_vcf_to_bed_ebm.py \
        --type {wildcards.filter_prefix} \
        --label {wildcards.label} \
        --input {input} \
        --output {output}
        """


rule concat_tsv_files:
    input:
        **{
            k: expand(rules.parse_label_vcf.output, label=k, allow_missing=True)
            for k in ["tp", "fp"]
        },
    output:
        label_dir / "{filter_prefix}_labeled.tsv",
    shell:
        """
        cp {input.tp} {output}
        tail -n+2 {input.fp} >> {output}
        """


## TODO add filtering rules here if we wish

################################################################################
# get annotations data

# TODO not sure if this is a snakemake bug or not, but I'm not "supposed" to be
# able to pass this string as a param because it has { and } (wildcards).
# Putting it in a tuple seems to make snakemake not care.
chr_regexp = "^chr[0-9]{1,2}$"


rule get_genome:
    output:
        annotations_src_dir / "genome.txt",
    params:
        columns="chrom,size",
        table="chromInfo",
        where_regexp=("chrom", chr_regexp),
        order_by="CAST(REPLACE(chrom,'chr','') as INT)",
        extra="-N",
    wrapper:
        "file://workflow/wrappers/mysql_select"


rule get_simreps_src:
    output:
        annotations_src_dir / "simple_repeats.tsv",
    params:
        table="simpleRepeat",
        where_regexp=("chrom", chr_regexp),
    wrapper:
        "file://workflow/wrappers/mysql_select"


rule get_repeat_masker_src:
    output:
        annotations_src_dir / "repeat_masker.tsv",
    params:
        columns="genoName,genoStart,genoEnd,repClass",
        table="rmsk",
        where_regexp=("genoName", chr_regexp),
    wrapper:
        "file://workflow/wrappers/mysql_select"


# As this file is being downloaded, strip the header, add a new column filled
# with 1's, and add our own header to identify this new column. When intersected
# with the feature tsv file, the 1's will be a binary representation of a
# variant being in a given mappability region (and non-overlaps will be empty,
# which presumably will be filled with 0's)
rule get_mappability_high_src:
    output:
        annotations_src_dir / "mappability_high.tsv",
    params:
        url=config["resources"]["annotations"]["mappability"]["high"],
        feature_name="mappability_high",
    shell:
        """
        echo 'chrom\tstart\tend\t{params.feature_name}' > {output}
        curl -L {params.url} | \
        gunzip -c | \
        sed 's/$/\t1/' | \
        tail -n+2 \
        >> {output}
        """


use rule get_mappability_high_src as get_mappability_low_src with:
    output:
        annotations_src_dir / "mappability_low.tsv",
    params:
        url=config["resources"]["annotations"]["mappability"]["low"],
        feature_name="mappability_low",


################################################################################
# process annotations (merge, filter, etc)


rmsk_classes = ["SINE", "LINE", "LTR", "Satellite"]


rule get_repeat_masker_classes:
    input:
        rules.get_repeat_masker_src.output,
    output:
        expand(
            annotations_tsv_dir / "repeat_masker_{cls}.tsv",
            cls=rmsk_classes,
        ),
    conda:
        "envs/bedtools.yml"
    params:
        outdir=lambda _, output: Path(output[0]).parent,
        classes=",".join(rmsk_classes),
    shell:
        """
        python workflow/scripts/get_rmsk_classes.py \
        -i {input} \
        -o {params.outdir} \
        -c {params.classes}
        """


rule get_simple_reps:
    input:
        src=rules.get_simreps_src.output,
        genome=rules.get_genome.output,
    output:
        annotations_tsv_dir / "merged_simreps.tsv",
    conda:
        "envs/bedtools.yml"
    shell:
        """
        python workflow/scripts/get_simple_repeats.py \
        -i {input.src} \
        -g {input.genome} \
        -o {output}
        """


# TODO add superdups and homopolymers


################################################################################
# add annotations


rule add_annotations:
    input:
        variants=rules.concat_tsv_files.output,
        tsvs=[
            rules.get_repeat_masker_classes.output,
            rules.get_simple_reps.output,
            rules.get_mappability_high_src.output,
            rules.get_mappability_low_src.output,
        ],
    output:
        annotated_dir / "{filter_prefix}.tsv",
    conda:
        "envs/bedtools.yml"
    shell:
        """
        python workflow/scripts/annotate.py \
        -i {input.variants} \
        -t {input.tsvs} \
        -o {output}
        """


################################################################################
# postprocess output


def get_postprocess_config(wildcards):
    f = config["ebm_runs"][wildcards.run_key]["feature_set"]
    return json.dumps(config["feature_sets"][f])


rule postprocess_output:
    input:
        lambda wildcards: expand(
            rules.add_annotations.output,
            filter_prefix=config["ebm_runs"][wildcards.run_key]["filter"],
        ),
    output:
        ebm_dir / "input.tsv",
    params:
        config=get_postprocess_config,
    shell:
        """
        python workflow/scripts/postprocess.py \
        -c '{params.config}' \
        -i {input} \
        -o {output}
        """


################################################################################
# run EBM
#
# assume that this will take care of test/train split, actual training, and
# pickling


def lookup_ebm_run(wildcards):
    e = config["ebm_runs"][wildcards.run_key]["ebm_settings"]
    return json.dumps({"settings": config["ebm_settings"][e]})


rule train_ebm:
    input:
        rules.postprocess_output.output,
    output:
        ebm_output_files,
    params:
        config=lookup_ebm_run,
        out_dir=str(ebm_dir),
    conda:
        "envs/ebm.yml"
    shell:
        """python workflow/scripts/run_ebm.py \
        -i {input} \
        -c '{params.config}' \
        -o {params.out_dir}
        """
