import pandas as pd
import json
import re
from functools import partial
import subprocess as sp
from os.path import join, basename, splitext, dirname
from snakemake.utils import min_version, validate
from scripts.common.config import lookup_run_json, lookup_config, lookup_run_config

min_version("6.12")


configfile: "config/config.yml"


validate(config, "schemas/config-schema.yml")


def get_git_tag():
    args = ["git", "describe", "--tags", "--abbrev=0", "--always"]
    tag = sp.run(args, capture_output=True).stdout.strip().decode()
    m = re.match("(v\d+\.\d+\.\d+)-\d+", tag)
    return tag if m is None else m[1]


git_tag = get_git_tag()
run_keys = list(config["runs"])

################################################################################
# output paths

conf_paths = config["paths"]

# downloaded files

resources_dir = conf_paths["resources"]

bench_dir = join(resources_dir, "bench")
annotations_data_dir = join(resources_dir, "annotations")

# computed output

results_dir = conf_paths["results"]

label_dir = join(results_dir, "labels")
rtg_dir = join(label_dir, "rtg")
preprocessing_dir = join(results_dir, "preprocessing")
annotated_dir = join(results_dir, "annotated")

ebm_dir = join(results_dir, "ebm", "{}_{{run_key}}".format(git_tag))
ebm_output_files = [
    join(ebm_dir, f)
    for f in [
        "model.pickle",
        "train_x.pickle",
        "train_y.pickle",
        "test_x.pickle",
        "test_y.pickle",
        "config.yml",
    ]
]

################################################################################
# main target
#
# Define what EBM models we want to run, and pin the output dirs to 'all'


rule all:
    input:
        expand(ebm_output_files, tag=git_tag, run_key=run_keys),


################################################################################
# VCF preprocessing


# rule get_vcf:
#     output:
#         join(resources_dir, "query.vcf.gz"),
#     params:
#         url=lookup_config(config, "resources", "query_url"),
#     shell:
#         "curl -o {output} {params.url}"


rule preprocess_vcf:
    input:
        join(resources_dir, "query.vcf.gz"),
        # rules.get_vcf.output,
    output:
        join(label_dir, "query_corrected_refcall.vcf.gz"),
    conda:
        "envs/samtools.yml"
    shell:
        """
        gunzip -c {input} | \
        sed -e '/.RefCall./ s/\.\/\./0\/1/g' | \
        sed -e '/.RefCall./ s/0\/0/0\/1/g' | \
        bgzip -c > {output}
        """


rule index_vcf:
    input:
        rules.preprocess_vcf.output,
    output:
        join(label_dir, "query_corrected_refcall.vcf.gz.tbi"),
    conda:
        "envs/samtools.yml"
    shell:
        "tabix -p vcf {input}"


################################################################################
# get reference sdf


ref_url = lookup_config(config, "resources", "ref", "sdf_url")


rule get_ref_sdf:
    output:
        directory(join(resources_dir, splitext(basename(ref_url))[0])),
    params:
        url=ref_url,
        dir=lambda _, output: dirname(output[0]),
    shell:
        "curl {params.url} | bsdtar -xf - -C {params.dir}"


################################################################################
# get benchmark files


# TODO use more unique names for these file so it is less likely in the future
# that we will change the config and have an old/wrong file that won't be
# overwritten by snakemake
rule get_bench_vcf:
    output:
        join(bench_dir, "bench.vcf.gz"),
    params:
        url=lookup_config(config, "resources", "bench", "vcf_url"),
    shell:
        "curl -o {output} {params.url}"


rule get_bench_tbi:
    output:
        join(bench_dir, "bench.vcf.gz.tbi"),
    params:
        url=lookup_config(config, "resources", "bench", "tbi_url"),
    shell:
        "curl -o {output} {params.url}"


rule get_bench_bed:
    output:
        join(bench_dir, "bench.bed"),
    params:
        url=lookup_config(config, "resources", "bench", "bed_url"),
    shell:
        "curl -o {output} {params.url}"


################################################################################
# VCF -> tsv


# rtg won't output to a directory that already exists, so do this weird temp
# file thing
rule get_vcf_labels:
    input:
        query_vcf=rules.preprocess_vcf.output,
        truth_vcf=rules.get_bench_vcf.output,
        truth_bed=rules.get_bench_bed.output,
        sdf=rules.get_ref_sdf.output,
        # not used on CLI but still needed
        query_tbi=rules.index_vcf.output,
        truth_tbi=rules.get_bench_tbi.output,
    output:
        tp=join(rtg_dir, "tp.vcf.gz"),
        fp=join(rtg_dir, "fp.vcf.gz"),
    conda:
        "envs/rtg.yml"
    params:
        extra="--ref-overlap --all-records",
        tmp_dir="/tmp/vcfeval",
        output_dir=rtg_dir,
    shell:
        """
        rtg vcfeval {params.extra} \
            -b {input.truth_vcf} \
            -e {input.truth_bed} \
            -c {input.query_vcf} \
            -o {params.tmp_dir} \
        -t {input.sdf}
        mv {params.tmp_dir}/* {params.output_dir}
        rm -r {params.tmp_dir}
        """


rule unzip_vcf_labels:
    input:
        join(rtg_dir, "{label}.vcf.gz"),
    output:
        join(label_dir, "{label}.vcf"),
    shell:
        "gunzip {input} -c > {output}"


rule parse_label_vcf:
    input:
        rules.unzip_vcf_labels.output,
    output:
        join(label_dir, "{label}.tsv"),
    shell:
        """
        python \
        workflow/scripts/parse_vcf_to_bed_ebm_indels_{wildcards.label}.py \
        --input {input} \
        --output {output}
        """


rule concat_tsv_files:
    input:
        **{k: expand(rules.parse_label_vcf.output, label=k) for k in ["tp", "fp"]},
    output:
        join(label_dir, "labeled.tsv"),
    shell:
        """
        cp {input.tp} {output}
        tail -n+2 {input.fp} >> {output}
        """


## TODO add filtering rules here if we wish

################################################################################
# get annotations data

# TODO not sure if this is a snakemake bug or not, but I'm not "supposed" to be
# able to pass this string as a param because it has { and } (wildcards).
# Putting it in a tuple seems to make snakemake not care.
chr_regexp = "^chr[0-9]{1,2}$"


rule get_genome_tsv:
    output:
        join(annotations_data_dir, "genome.txt"),
    params:
        columns="chrom,size",
        table="chromInfo",
        where_regexp=("chrom", chr_regexp),
        order_by="CAST(REPLACE(chrom,'chr','') as INT)",
        extra="-N",
    wrapper:
        "file://workflow/wrappers/mysql_select"


rule get_simreps_tsv:
    output:
        join(annotations_data_dir, "simple_repeats.tsv"),
    params:
        table="simpleRepeat",
        where_regexp=("chrom", chr_regexp),
    wrapper:
        "file://workflow/wrappers/mysql_select"


rule get_repeat_masker_tsv:
    output:
        join(annotations_data_dir, "repeat_masker.tsv"),
    params:
        columns="genoName,genoStart,genoEnd,repClass",
        table="rmsk",
        where_regexp=("genoName", chr_regexp),
    wrapper:
        "file://workflow/wrappers/mysql_select"


################################################################################
# add annotations


rule add_superdups:
    input:
        variants=rules.concat_tsv_files.output,
    output:
        join(annotated_dir, "1_added_superdups.tsv"),
    conda:
        "envs/bedtools.yml"
    shell:
        """cat {input.variants} > {output}"""


rule add_homopolymers:
    input:
        variants=rules.add_superdups.output,
    output:
        join(annotated_dir, "2_added_homopolymers.tsv"),
    conda:
        "envs/bedtools.yml"
    shell:
        """cat {input.variants} > {output}"""


rule add_simple_reps:
    input:
        variants=rules.add_homopolymers.output,
        annotations=rules.get_simreps_tsv.output,
        genome=rules.get_genome_tsv.output,
    output:
        join(annotated_dir, "3_added_simreps.tsv"),
    conda:
        "envs/bedtools.yml"
    shell:
        """
        python workflow/scripts/annotate_simple_repeats.py \
        -i {input.variants} \
        -g {input.genome} \
        -s {input.annotations} \
        -o {output}
        """


rule add_repeat_masker:
    input:
        variants=rules.add_simple_reps.output,
        annotations=rules.get_repeat_masker_tsv.output,
    output:
        join(annotated_dir, "4_added_repeat_masker.tsv"),
    conda:
        "envs/bedtools.yml"
    shell:
        """
        python workflow/scripts/annotate_repeat_masker.py \
        -i {input.variants} \
        -r {input.annotations} \
        -o {output}
        """


# rule add_mappability:
#     input:
#         add_repeat_masker.output,
#     output:
#         join(preprocessing_dir, "added_mappability.tsv"),


################################################################################
# postprocess output


def get_postprocess_config(wildcards):
    c = lookup_run_config(config, wildcards.run_key)
    return json.dumps(c["features"])


rule postprocess_output:
    input:
        rules.add_repeat_masker.output,
    output:
        join(ebm_dir, "input.tsv"),
    params:
        config=get_postprocess_config,
    shell:
        """
        python workflow/scripts/postprocess.py \
        -c '{params.config}' \
        -i {input} \
        -o {output}
        """


################################################################################
# run EBM
#
# assume that this will take care of test/train split, actual training, and
# pickling


rule train_ebm:
    input:
        rules.postprocess_output.output,
    output:
        ebm_output_files,
    params:
        config=lambda wildcards: lookup_run_json(config, wildcards.run_key),
        out_dir=ebm_dir,
    conda:
        "envs/ebm.yml"
    shell:
        """python workflow/scripts/run_ebm.py \
        -i {input} \
        -c '{params.config}' \
        -o {params.out_dir}
        """
